{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIM12: Find the Kermit\n",
    "Craig Lincoln (11828331) and Kent Rexha (TODO)\n",
    "\n",
    "## Approach\n",
    "Out approach is to use a pre-trained neural net and feed both audio and image features into the same model. The proposed neural net would be ImageAI's ResNet which has an input size of 224,244,3. Obviously, feeding images is no problem. For the audio features we plan to add them to the images. Currently, the audio features are MFCC and Chroma from Librosa and will have dimensions, ~20 and ~12 respectively. This maybe something we vary as well add more features.\n",
    "\n",
    "- Open questions: How will we divide training, test and validation sets?\n",
    "- Do we want to add more audio features and which?\n",
    "- How do we add the audio features explicitly, eg, just to the first color channel or the same features vector to all three and what do we do with in the case of augmentation (allow the audio features to be modified or always include them last?)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}